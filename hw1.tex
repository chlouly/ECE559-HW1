\documentclass[11pt]{article}
%
\renewcommand{\baselinestretch}{1}
\topmargin=-0.5truein
\textheight=9.0truein
\oddsidemargin=0.0truein
\textwidth=6.5truein
%
\pagestyle{empty}

\input{macros.tex}
\usepackage{comment}
\usepackage{fancyhdr}
\usepackage{enumitem}
\usepackage{dsfont}

\begin{document}

\pagestyle{fancy}
\fancyhf{} % clear default
\fancyhead[L]{Chris Louly - HW 1 Submission} % left side header
\fancyhead[C]{}            % center (empty)
\fancyhead[R]{\thepage}    % right side = page number


\begin{center}
{\Large {\bf EECS 559 HW 1}}
\end{center}

\vspace{.05in}



\vspace{.1in}

%%% --- QUESTION 1 --- %%%
\textbf{QUESTION 1:}
\begin{enumerate}[label=(\alph*)]
    \item   Given $\bA \in \mathbb{R}^{m \times n}$ and $\by \in \mathbb{R}^{m}$, show that the basis pursuit problem:
            \[
                \min_{\bx \in \mathbb{R}^n} \norm{\bx}_1 \;\; \textbf{s.t.} \;\; \bA \bx = \by
            \]
            is a convex problem, and recast it as a \textit{linear program} in the standard form. \newline

            \textbf{SOLUTION:}
            Firstly, we can show that this is a convex optimization problem by showing that both the objective function and constraints are convex.
            We start by examining the objective function:
            \begin{align*}
                \norm{\bx}_1    &=  \sum_{i} \abs{x_i} \\
                                &=  \sum_{i} \max\{x_i, - x_i\}
            \end{align*}

            We know that $x_i$ and $-x_i$ are linear functions, therefore convex, thus the pointwise maximum of these two functions is convex.
            Since the objective function is a nonnegative sum of these pointwise maxima, it is also convex. 
            The constraint:
            \[
                \bA \bx = \by
            \]

            is linear, therefore convex. which means that this \textit{is a convex optimization problem}. \newline

            We can now recast this as a linear program in the standard form. We start by defining $\bx_+$ and $\bx_-$ as follows:
            \begin{align*}
                \bx_+ &\triangleq \sum_i x_i \mathds{1}_{\{x_i \geq 0\}} \be_i \\
                \bx_- &\triangleq \sum_i x_i \mathds{1}_{\{x_i < 0\}} \be_i
            \end{align*}

            % Where $\be_i$ is the $i^\textit{th}$ cannonical basis vector of $\mathbb{R}^n$. Noitce that:
            % \begin{align*}
            %     \bx = \bx_+ - \bx_-
            % \end{align*}
            We can define:
            \begin{align*}
                \bz \triangleq \begin{bmatrix}
                    \bx_+ \\
                    \bx_-
                \end{bmatrix}
            \end{align*}

            And clearly:
            \begin{align*}
                \norm{\bx}_1    &= \sum_{i=0}^n \abs{x_i} \\
                                &= \sum_{i=0}^n x_i \mathds{1}_{\{x_i \geq 0\}} + \sum_{i=0}^n x_i \mathds{1}_{\{x_i < 0\}} \\
                                &= \mathbf{1}_n^T \bx_+ + \mathbf{1}_n^T \bx_- \\
                                &= \begin{bmatrix}
                                    \mathbf{1}_n^T & \mathbf{1}_n^T 
                                \end{bmatrix} \bz
            \end{align*}

            We can now recast the problem as:
            \begin{align*}
                \min_{\bx \in \mathbb{R}^n} \bc^T \bz \;\; \textbf{s.t.} \;\; \ba_i^T \bz = \bb_i \;\; \forall i \in \left[n\right]
            \end{align*}

            Where:
            \begin{align*}
                \bc \triangleq \begin{bmatrix}
                    \mathbf{1}_n \\
                    \mathbf{1}_n
                \end{bmatrix}, \;\;
                \ba_i \triangleq \bM_{\left[i, :\right]}, \;\;
                \bM \triangleq \begin{bmatrix}
                    \bA & -\ba
                \end{bmatrix}
            \end{align*}

\end{enumerate}
\newpage


%%% --- QUESTION 2 --- %%%
\textbf{QUESTION 2:} Consider the following unconstrained optimization problem:

\[
    \min_{\bx \in \mathbb{R}^n} f(\bx)
\]

Where $f : \mathbb{R}^n \mapsto \mathbb{R}$ is a convex function.

\begin{enumerate}[label=(\alph*)]
    \item   Show that if $\bx_0$ is a local minimizer of $f$,  then it is also a global solution (need not to be unique).
            Moreover, if $f$ is \textit{strictly} convex, then show that the solution is unique (if exists). \newline

            \textbf{PROOF:} (By contradiction) Let $\bx_0$ be a local minimizer of $f$, meaning that $\exists \epsilon > 0$ such that:

            \[
                f(\bx_0) \leq f(\bx) \;\; \forall \bx \in \mathcal{B}(\bx_0, \epsilon) \triangleq \{\bx \in \mathbb{R}^n \; | \; \norm{\bx_0 - \bx}_2 \leq \epsilon\}
            \]

            Assume $\exists \bx_* \in \mathbb{R}^n$ such that $f(\bx_*) < f(\bx_0)$. $\forall \epsilon$ sufficiently small, $\exists \alpha \in (0, 1)$ such that:
            \begin{align*}
            \by \triangleq \alpha \bx_0 + (1 - \alpha) \bx_* \in \mathcal{B}(\bx_0, \epsilon)
            \end{align*}

            And by the convexity of $f$:
            \begin{align*}
                f(\by) \leq \alpha f(\bx_0) + (1 - \alpha) f(\bx_*)
            \end{align*}

            Since $f(\bx_*) < f(\bx_0)$, we can write:
            \begin{align*}
                f(\by) < \alpha f(\bx_0) + (1 - \alpha) f(\bx_0) = f(\bx_0)
            \end{align*}

            Which is when we arrive at our contradiction. 
            Since for any arbitrary $\epsilon > 0$ there is an element $\by \in \mathcal{B}(\bx_0, \epsilon)$ such that $f(\bx_0) > f(\by)$, our assumption is false.
            Therefore, any local minimum is a global minimum. \qed

            Let's now assume that $f$ is \textit{strictly convex}. Assuming it exists, we can show the uniqueness of the global minimizer as follows. \newline
            
            \textbf{PROOF:} (By contradiction) Let $\bx_0$ is a local minimizer of $f$. Since strict complexity implies convexity, we know that $\bx_0$ is also a global minimizer (from the previous argument).
            \begin{align*}
                f(\bx_0) \leq f(\bx) \;\; \forall \bx \in \mathbb{R}^n
            \end{align*}

            Assume $\exists \bx_* \in \mathbb{R}^n$ such that $f(\bx_0) = f(\bx_*)$. By strict convexity of $f$, $\forall \alpha \in (0, 1)$:
            \begin{align*}
                f(\alpha \bx_0 + (1 - \alpha) \bx_*) < \alpha f(\bx_0) + (1 - \alpha) f(\bx_*) = f(\bx_0)
            \end{align*}

            Therefore there is an element in the domain that achieves a lower value than our global minimizer, which is our contradiction. 
            Therefore our assumption is wrong. Given that $f$ is \textit{strictly convex} the local minimizer $\bx_0$ is a \textit{unique global} minimizer. \qed \newpage
\end{enumerate}

\textbf{QUESTION 2:} Continued
\begin{enumerate}[label=(\alph*), start=2]
    \item   Furthermore, suppose $f \in \mathcal{C}^1$ (i.e., continuously differentiable). Then show that $\bx_0$ is a \textit{global} minimizer of $f$ iff:
            \begin{align*}
                \nabla f(\bx_0) = \mathbf{0}
            \end{align*}

            \textbf{PROOF:} ($\Rightarrow$) (By contradiction) Let $\bx_0$ be a global minimum of $f$, we assume $\nabla f(\bx_0) \neq \mathbf{0}$. Since $f$ is convex, it satisfies the first order condition, $\forall \by \in \mathbb{R}^n$:
            \begin{align*}
                f(\by) \geq f(\bx_0) + (\by - \bx_0)^T \nabla f(\bx_0)
            \end{align*}

            We can specifically choose some arbitrary $\by$:
            \begin{align*}
                \by(t) = \bx_0 + t\nabla f(\bx_0) \;\; \forall t \in \mathbb{R}
            \end{align*}

            We apply this to the first order condition to get:
            \begin{align*}
                f(\bx_0 - t\nabla f(\bx_0)\b) \geq f(\bx_0) + t \norm{\nabla f(\bx_0)}^2
            \end{align*}

            $\exists t$ s.t.:
            \begin{align*}
                f(\bx_0 - t\nabla f(\bx_0)\b) \leq f(\bx_0) + t \norm{\nabla f(\bx_0)}^2
            \end{align*}

            Unless $\norm{\nabla f(\bx_0)}^2 = 0$ which is possible iff $\nabla f(\bx_0) = \mathbf{0}$ \newline

            \textbf{PROOF:} ($\Leftarrow$) Assume $\nabla f(\bx_0) = \mathbf{0}$. Since $f$ is convex, it satisfies the first order condition, $\forall \by \in \mathbb{R}^n$:
            \begin{align*}
                f(\by)  &\geq   f(\bx_0) + (\by - \bx_0)^T \nabla f(\bx_0) \\
                        &=      f(\bx_0) + (\by - \bx_0)^T \mathbf{0} \\
                        &=      f(\bx_0)
            \end{align*}

            So $\forall \by \in \mathbb{R}^n$, $\nabla f(\bx_0) = \mathbf{0}$ implies that $\bx_0$ is a global minimum of $f$.
            
\end{enumerate}

\newpage


%%% --- QUESTION 3 --- %%%
\textbf{QUESTION 3:}
Given $f(\bx), f_1(\bx), f_2(\bx), \dots, f_m(\bx)$ are convex on $\mathbb{R}^m$, show that:

\begin{enumerate}[label=(\alph*)]
    \item   If $\alpha_i \geq 0 \;\; \forall i \in [n]$, then $g(\bx) \triangleq \sum \alpha_i f_i(\bx)$ is a convex function.
    
            \textbf{SOLUTION:} Consider some $\beta \in [0, 1]$ and $\bx, \by \in \mathbb{R}^n$. By the confexity of $f_i$:
            \begin{align*}
                \beta f_i(\bx) + (1 - \beta) f_i(\by) \geq f_i(\beta \bx + (1 - \beta) \by)
            \end{align*}

            Which implies:
            \begin{align*}
                \sum \alpha_i \left[\beta f_i(\bx) + (1 - \beta) f_i(\by)\right] &\geq \sum \alpha_i f_i(\beta \bx + (1 - \beta) \by) \\
                \beta \sum \alpha_i f_i(\bx) + (1 - \beta) \sum \alpha_i f_i(\by) &\geq g(\beta \bx + (1 - \beta) \by) \\
                \beta g(\bx) + (1 - \beta) g(\by) &\geq g(\beta \bx + (1 - \beta) \by) \;\; \forall \beta \in [0,1] \;\; \forall \bx, \by \in \mathbb{R}^n
            \end{align*}

            Therefore $g$ is a convex function.

    \item   $g(\bx) \triangleq \max \{f_1(\bx), \dots, f_n(\bx)\}$ is convex:

            \textbf{SOLUTION:} Consider some $\beta \in [0, 1]$ and $\bx_1, \bx_2 \in \mathbb{R}^n$.
            \begin{align*}
                f_i(\beta \bx_1 + (1-\beta) \bx_2) &\leq \beta f_i(\bx_1) + (1-\beta) f_i(\bx_2) \\
                \max_i f_i(\beta \bx_1 + (1-\beta) \bx_2) &\leq \max_i \left[\beta f_i(\bx_1) + (1-\beta) f_i(\bx_2)\right]
            \end{align*}

            Notice that $\forall a_i, b_i \in \mathbb{R}$, $\max\left\{a_i + b_i\right\} \leq \max\left\{a_i\right\} + \max\left\{b_i\right\}$. Therefore:
            \begin{align*}
                \max_i f_i(\beta \bx_1 + (1-\beta) \bx_2) &\leq \beta \max_i f_i(\bx_1) + (1 - \beta) \max_i f_i(\bx_2) \\
                g(\beta \bx_1 + (1-\beta) \bx_2) &\leq \beta g(\bx_1) + (1 - \beta) g(\bx_2)
            \end{align*}

            Therefore the convexity of $f$ implies the convexity of $g$.


    \item   Given $\bA \in \mathbb{R}^{n \times m}$, $\bx \in \mathbb{R}^m$ and $\bb \in \mathbb{R}^n$, $g(\bx) \triangleq f(\bA \bx + \bb)$ is convex. 
    
            \textbf{SOLUTION:} Consider $\bx_1$, $\bx_2$ $\in \mathbb{R}^m$, and $\alpha \in [0,1]$
            \begin{align*}
                f(\bA (\alpha \bx_1 + (1-\alpha)\bx_2) + \bb) &= f(\alpha(\bA \bx_1 + \bb) + (1-\alpha)(\bA \bx_2 + \bb))
            \end{align*}

            By the convexity of $f$:
            \begin{align*}
                f(\bA (\alpha \bx_1 + (1-\alpha)\bx_2) + \bb) &\leq \alpha f(\bA \bx_1 + \bb) + (1-\alpha)f(\bA \bx_2 + \bb) \\
                g(\alpha \bx_1 + (1-\alpha)\bx_2) &\leq \alpha g(\bx_1) + (1-\alpha)g(\bx_2)
            \end{align*}

            Therefore the convexity of $f$ implies the convexity of $g$.


\end{enumerate}



\newpage

%%% --- QUESTION 4 --- %%%
\textbf{QUESTION 4:}
Show that $f : \mathbb{R}^n \mapsto \mathbb{R}$ is convex iff for all integers $m \geq 2$:
\begin{align}
    f\left(
        \sum_{i = 1}^{m} \lambda_i \bx_i
    \right) \leq \sum_{i = 1}^{m} \lambda_i f(\bx_i) \tag{$\star$}
\end{align}

Where $x_i \in \mathbb{R}^n$ and $\lambda_i \geq 0$ $\forall i \in \left[m\right]$ satisfying $\sum \lambda_i = 1$. \newline

\textbf{PROOF:} (by induction) Consider the base case ($m = 2$). The expression above boils down to the following:
\begin{align*}
    f(\lambda_1 \bx_1 + \lambda_2 \bx_2) \leq \lambda_1 f(\bx_1) + \lambda_2 f(\bx_2)
\end{align*}

Notice that $\lambda_1 + \lambda_2 = 1$ is equivalent to $\lambda_2 = 1 - \lambda_1$. And since $\lambda_2 \geq 0$ must be true, we know that $\lambda_1 \in [0,1]$. After substituting we get the definition of convexity:
\begin{align*}
    f(\lambda_1 \bx_1 + (1 - \lambda_1) \bx_2) \leq \lambda_1 f(\bx_1) + (1 - \lambda_1) f(\bx_2)
\end{align*}

So the base case ($m = 2$) the inequality ($\star$) is true iff $f$ is convex. \newline

Now let's assume ($\star$) is true for some fixed value ($m = k \geq 2$):
\begin{align*}
    f\left(
        \sum_{i = 1}^{k} \lambda_i \bx_i
    \right) \leq \sum_{i = 1}^{k} \lambda_i f(\bx_i)
\end{align*}

W.l.o.g., we can express $\bx_k$ as follows:
\begin{align*}
    \bx_k = \alpha \by_k + (1 - \alpha) \by_{k+1} \;\; \text{where} \;\; \alpha \in [0, 1]
\end{align*}

Where $\by_k$ and $\by_{k+1}$ are arbitrary vectors in $\mathbb{R}^n$. We can rewrite ($\star$) as follows:
\begin{align*}
    f\left(
        \sum_{i = 1}^{k - 1} \lambda_i \bx_i + \lambda_k \alpha \by_k + \lambda_k (1 - \alpha) \by_{k + 1}
    \right) \leq \sum_{i = 1}^{k - 1} \lambda_i f(\bx_i) + \lambda_k f(\alpha \by_k + (1 - \alpha) \by_{k+1})
\end{align*}

And by the convexity of $f$:
\begin{align*}
    \lambda_k f(\alpha \by_k + (1 - \alpha) \by_{k+1}) \leq \lambda_k \alpha f(\by_k) + \lambda_k (1 - \alpha) f(\by_{k+1})
\end{align*}

Which leaves us with:
\begin{align*}
    f\left(
        \sum_{i = 1}^{k - 1} \lambda_i \bx_i + \lambda_k \alpha \by_k + \lambda_k (1 - \alpha) \by_{k + 1}
    \right) \leq \sum_{i = 1}^{k - 1} \lambda_i f(\bx_i) + \lambda_k \alpha f(\by_k) + \lambda_k (1 - \alpha) f(\by_{k+1})
\end{align*}

And since $\sum_{i=1}^{k-1} \lambda_i + \lambda_k(\alpha + 1 - \alpha) = 1$, the $m=k$ case being true implies that the $m=k+1$ case is aslo true. 
Therefor by mathematical induction, $\forall m \geq 2$, $\forall i \in \left[m\right]$, $\forall \lambda_i \geq 0$, $\forall \bx_i \in \mathbb{R}^n$:
\begin{align*}
    f: \mathbb{R}^n \mapsto \mathbb{R} \; \text{convex} \;\; \iff \;\; f\left(
        \sum_{i = 1}^{m} \lambda_i \bx_i
    \right) \leq \sum_{i = 1}^{m} \lambda_i f(\bx_i) \;\;  \text{where} \;\; \sum\lambda_i = 1
\end{align*}
\qed
\newpage

%%% --- QUESTION 5 --- %%%
\textbf{QUESTION 5:}
\begin{enumerate}[label=(\alph*)]
    \item   Show that the sequence $x_k = 1 + \left(\frac{1}{2}\right)^{2^k}$ is Q-quadratically convergent to 1. \newline
            \textbf{SOLUTION:} We know that $\lim_{k \mapsto \inf} x_k = 1$, consider:
            \begin{align*}
                \norm{1 + \left(\frac{1}{2}\right)^{2^k} - 1}_2 &\leq \gamma \norm{1 + \left(\frac{1}{2}\right)^{2^{k-1}} - 1}_2^2 \\
                \left(\frac{1}{2}\right)^{2^k} &\leq \gamma \left[\left(\frac{1}{2}\right)^{2^{k-1}}\right]^2\\
                \left(\frac{1}{2}\right)^{2^k} &\leq \gamma \left(\frac{1}{2}\right)^{2^{k}}
            \end{align*}

            Which is true $\forall \gamma \leq 1$, $\forall k \geq 1$, therefore the sequence $\left\{ x_k \right\}$ is Q-quadratically convergent.

    \item   Does the sequence $x_k = \frac{1}{k!}$ converge Q-superlinearly? Q-quadratically? \newline
            \textbf{SOLUTION:} We know that $\lim_{k \mapsto \inf} x_k = 0$, consider:
            \begin{align*}
                \norm{\frac{1}{k!}}_2 &\leq \gamma \norm{\frac{1}{(k-1)!}}_2^p \\
                \frac{1}{k!} &\leq \gamma \left[\frac{1}{(k-1)!}\right]^p \\
                \frac{((k-1)!)^{p-1}}{k} &\leq \gamma \\
                (p-1)\log((k-1)!) - \log(k) &\leq \log(\gamma) \\
                (p-1)\sum_{i = 1}^{k-1}\log(i) - \log(k) &\leq \log(\gamma)
            \end{align*}

            The sequence above is clearly only upper bounded if $p-1 \leq 0$, therefore $\left\{ x_k \right\}$ is Q-convergent with $p \leq 1$.
            So the sequence is neither Q-quadratically nor Q-superlinearly convergent.
\end{enumerate}
\newpage

\textbf{QUESTION 5:} Continued
\begin{enumerate}[label=(\alph*), start=3]
    \item   Consider the sequence $\left\{x_k\right\}$ defined by
            \begin{align*}
                x_k = \begin{cases}
                    \left(\frac{1}{4}\right)^{2^k} & k \; even \\
                    \frac{x_{k-1}}{k} & k \; odd
                \end{cases}
            \end{align*}
            Is this sequence Q-superlinearly convergent? Q-quadratically convergent? R-quadratically convergent? Justify your answer. \newline
            \textbf{SOLUTION:} We know that $\lim_{k \mapsto \inf} x_k = 0$, consider the case where $k$ is odd:
            \begin{align*}
                \norm{x_k}_2 &\leq \gamma \norm{x_{k-1}}_2^p \\
                \frac{x_{k-1}}{k} &\leq \gamma x_{k-1}^p \\
                \frac{1}{k} \left(\frac{1}{4}\right)^{(1-p)2^k} &\leq \gamma
            \end{align*}

            There can only be such $\gamma$ if $p \leq 1$, therefore $\left\{ x_k \right\}$ is neither Q-quadratically nor Q-superlinearly convergent.
            Now consider the sequence $\left\{ \rho_k \right\}$ defined as follows:
            \begin{align*}
                \rho_k = \left(\frac{1}{4}\right)^{2^k} \;\; \forall k \in \mathbb{N}
            \end{align*}

            Notice that:
            \begin{align*}
                \rho_k &\geq x_k \;\; \forall k \in \mathbb{N} \\
                \rho_k &\geq \norm{x_k - 0}_2
            \end{align*}

            We know from (a) that $\left\{ \rho_k \right\}$ is Q-quadratically convergent, therefore $\left\{ x_k \right\}$ is R-quadratically convergent.
\end{enumerate}

\newpage

\end{document}


